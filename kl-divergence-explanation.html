<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, minimumscale=1.0, maximum-scale=1.0" />
  <title>Home of Sarthak</title>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <link rel="stylesheet" type="text/css" href="./css/style.css" />
  <link rel="stylesheet" type="text/css" href="./css/pygments.css" />
</head>
<body>
  <div id="container">
    <div id="header">
      <div id="righty">
        <div id="nom">
          <a href="/"><h1>Sarthak Munshi</h1></a>
        </div>
        <div id="links">
          <p>
            <a href="https://stackoverflow.com/users/3301488/saruftw">stackoverflow</a> -
            <a href="https://twitter.com/saruftw">twitter</a> -
            <a href="https://pastebin.com/raw/yVZ45Tfn">gpg key</a>
            <!--- <a href="files/brice-fernandes-cv.pdf">CV(pdf)</a> -->
            <span class="semail" >
              <a href="#">
                smunshi [at] cmu [dot] edu
              </a>
            </span>
          </p>
        </div> <!-- /links -->
      </div><!-- /righty -->
    </div><!-- /header -->
    <div id="content">

<!-- end top -->

<section>
  <h2>What is the Kullback-Leibler divergence?</h2><h6>Feb 15, 2018</h6>
  <article class="post-content">
    <p>Before we understand what the <em>Kullback-Leibler (or KL) divergence</em> is, we need to understand what the term <em>entropy</em> means or represents in the field of information theory and statistics.</p>

<h2 id="entropy">Entropy</h2>

<p>This term first found it’s way into a myriad number of statistical applications (ignoring the relation to thermodynamics) through a paper published by the legendary <a href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> in 1948 called “A Mathematical Theory of Communication”[<a href="http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf">pdf</a>]. The main problem being targeted here was the communication of bits in a network. With the advent of the digital age in the mid-1900s, it became increasingly crucial to efficiently send bits(or information) from a source to a destination.</p>

<p>We know that a 2 digit binary number can represent 2<sup>2</sup> pieces of information. If we add another digit to it, we get 2<sup>3</sup> ways of representing the same information. In a nutshell, a single bit can reduce our uncertainty of knowing something by a factor of 2 or we get twice the number of ways to represent the same information. However in our bit sequence, there will be certain combinations which aren’t used at all or maybe some of them represent an error code of some sort. What if we used a 3-bit uniform sequence to represent 6 pieces of information? Not that efficient, right? Wouldn’t it be better if we devise a code to reduce the number of bits and preserve only the critical information at the same time?</p>

<p>This is where <em>entropy</em> comes in. Looking at the information, it tells us the ideal number of bits required to represent a particular information for transmission.</p>

<p>Let’s consider an example.</p>

<p align="center">
<img src="https://spaceplace.nasa.gov/glossary/en/searching-earth.en.png" alt="" />
</p>
<p>Suppose, you are an official at NASA and your job is to receive the temperature levels of a particular planet from a fellow astronaut stationed at the International Space Station. Years of observation of this planet’s temperatures levels have concluded that it can only have 8 different levels of it.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">possible_temperatures_levels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">162</span><span class="p">,</span> <span class="mi">174</span><span class="p">,</span> <span class="mi">180</span><span class="p">]</span>
</code></pre></div></div>

<p>Each of them has a certain probability of occurring attached to it. It’s clear that some of them occur more commonly than the others.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># (of course, they all sum up to 1)</span>
<span class="n">temperature_probs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
</code></pre></div></div>
<p>Let’s consider a trivial example at first.</p>

<p>Suppose, the astronaut wants to convey today’s temperature to the NASA official. He could do it by sending the string - “Today’s temperature is 105°C”. This string is 28 characters or 8 x 28 = 224 bits long. Do we really need these many bits? If we shorten the string to just “105”, we still require 3 x 8 = 24 bits to convey the information. Since we have 8 different temperature levels, the ideal way to go about it would be to use 3 bits where each 3-bit pattern represents a specific temperature level. Using 24 bits to represent information worth only 3 bits is a sin. Also, note that if a receiver successfully receives the information of the temperature being 105°C, it would have reduced its uncertainty(or confusion) by a factor of 8(as mentioned earlier).</p>

<p>Well, this was easy. But how do we find the magic number of 3 mathematically? That is easy too, right? Just take a <script type="math/tex">\log_2</script> of the number of available unique pieces of information (eg. <script type="math/tex">\log_2 8 = 3</script>) and we’re sorted. Well, no!</p>

<p>That would only work if the probabilities for all the temperatures levels is same. It is to be noted that in the case of uneven probabilities(as in our example), we can further save up on the number of bits being transferred and that is what entropy addresses. The intuition here is that we don’t need as many bits for something that occurs much more frequently compared to something that occurs rarely. If 105°C occurs 60% of the time and 162°C occurs only 2% of the time, then it doesn’t makes sense to use the same bit code(encoding) for representing both these pieces of information.</p>

<p>I hope you understand the problem now.</p>

<p>If the astronaut sends us a continuous stream of information about the temperatures such as,</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100,105,110,162,100......and so on.
</code></pre></div></div>
<p>where each number/temperature is obviously represented by some kind of a bit code.</p>

<p>The frequency of occurrence of each temperature in this stream is dependent on the given probability distribution. The NASA official gets puzzled and ponders over a question - “How many bits or guesses do I need on an average to correctly predict a temperature level in a stream?”. Rephrasing his concerns in layman’s terms, he wants to know that on an average, how many questions does he need to ask to correctly predict or tell the temperature value currently received by him? This is known as entropy. The average number of times the NASA official will encounter uncertainty before correctly predicting something.</p>

<p>Mathematically, it is represented as</p>

<p>
\(H(p) =
\begin{cases}
- \sum_{i} p_i\log_2 p_i  & \text{for a discrete set of probabilities} \\\\
- \int_{-\infty}^{\infty} p_i\log_2 p_i & \text{for a continuous set of probabilities}
\end{cases}\)</p>

<p>Throughout this post, we deal with a discrete set of probabilities.</p>

<p>One may argue that in order to find the number of “guesses” to rightly predict something from the stream, we can just multiply the probabilities. For example,</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">([</span><span class="mf">0.30</span><span class="p">,</span> <span class="mf">0.40</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">])</span>
<span class="c"># result = 0.0000000012600000000000002</span>
</code></pre></div></div>
<p>However, calculating the product serves no purpose as the result is extremely tiny and doesn’t provide a lot of insights. Multiplication is also an expensive operation. A simple way to tackle this is to convert this into a sum by using <script type="math/tex">\log</script>. Utilizing the property,</p>

<p>\(\log(xy) = \log(x) + \log(y)\)</p>

<p>we get,</p>

\(\log_2 (0.30 * 0.40 * 0.05 * 0.06 * 0.07 * 0.02 * 0.05 * 0.05) \)
   \( = \log_2 (0.0000000012600000000000002) \)

  <p><br /></p>

\( \implies \log_2 0.30 + \log_2 0.40 + \ldots + \log_2 0.05 \) 
\( = \log_2 (0.0000000012600000000000002) \)
<p><br /></p>
\( \implies \log_2 0.30 + \log_2 0.40 + \ldots + \log_2 0.05 = -29.5639291203\)
<p><br /></p>
\( \implies - (\log_2 0.30 + \log_2 0.40 + \ldots + \log_2 0.05 ) = 29.5639291203\)
<p><br /></p>
\( \text{or}\)
<p><br /></p>
\( - \sum_{i} \log_2 p_i = 29.5639291203\\ \)

<p>Entropy means the average number of bits/guesses. Had the probabilities been similar for all the temperatures, we could have just done</p>

<script type="math/tex; mode=display">- \frac{1}{8}(\sum_{i} \log_2 (\frac{1}{8})) = -\log_2 (\frac{1}{8}) = 3.0 \\</script>

<p>But since we already have a given probability distribution, we multiply each of the <script type="math/tex">\log_2 p_i</script> terms with their respective probabilities. Why? Because when we calculate an average of a set of numbers, we assume that each of the numbers in the set are equally likely to be closer to the average. But when the probabilities for each number in the set are mentioned, things are done differently.</p>

<script type="math/tex; mode=display">- (0.30(\log_2 0.30) + 0.40(\log_2 0.40) + 0.05(\log_2 0.05) + 0.06(\log_2 0.06) + 0.07(\log_2 0.07) + \\0.02(\log_2 0.02) + 0.05(\log_2 0.05) + 0.05(\log_2 0.05)) \\
\text{or}\\
- \sum_{i} p_i\log_2 p_i = 2.323115964316818 = \text{~}2.3\\</script>

<p>Obtaining an entropy roughly equal to 2.3 tells us that we need to ask questions on an average of 2.3 times or need 2.3 bits per information in order to clearly determine what the message stands for and efficiently transmit it. This also means that we only have 2.3 bits of useful information out of the 3 bits if we would have used the coding scheme described earlier (we’d be wasting 0.7 bits if we used a 3-bit pattern on this sample).</p>

<p>For folks who can’t get their head around what 2.3 bits would look like, don’t think in terms of a binary number but in terms of the portion or part of an information. Let me propose 2 ways of looking at it which will hopefully make this more lucid.</p>

<ul>
  <li>
    <p>Considering the planetary temperature example mentioned above, we know that we have 8 different kinds of information and 2.3 is our entropy. Now if we use 2.3 bits per information, our message would be 2.3 x 8 = 18.4 bits long which is not possible. But if we send the message 10 times, we can do it in 184 bits in contrast to the 3 x 8 x 10 = 240 bits. Scale of transmission is tackled here.</p>
  </li>
  <li>
    <p>Consider a state election where 4 candidates (3 women and 1 man) are contesting. Suppose we find some dirt on the only male candidate and he get’s eliminated from the race. We still have 3 candidates we can vote for. The dirt certainly gave me more than 0 bits of information because I know more than I knew before. But at the same time, it did not give me 1-complete bit of information because if that would have happened, I would have reduced my uncertainty by a factor of 2 and be left with only 2 candidates in total (refer to the third paragraph in this post).</p>
  </li>
</ul>

<h2 id="cross-entropy">Cross-Entropy</h2>

<p>Since we now know what entropy is, cross entropy should be fairly easy to understand. A Wikipedia lift tells us that,</p>

<blockquote>
  <p>Cross entropy between two probability distributions  p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an “unnatural” probability distribution q, rather than the “true” distribution  p.</p>
</blockquote>

<p>In simple terms,</p>

<p>Given that <script type="math/tex">H(p) = - \sum_{i} p_i\log_2 p_i</script> represents entropy, we can say that <script type="math/tex">H(p, q) = - \sum_{i} p_i\log_2 q_i</script> represents cross entropy. You ask how? Well, consider you have 2 probability distributions <em>p</em> and <em>q</em> where <em>p</em> represents a true distribution and <em>q</em> represents a predicted distribution. Both <em>p</em> and <em>q</em> are distributions over the same set of events. Imagine the last step of a neural network wherein you obtain a probability distribution(or <em>q</em>) and then compare it with a distribution you already know beforehand(or <em>p</em>) where both the distributions describe the same set of classes such animals, plants, cars, or whatever your model is trained on.</p>

<p>In entropy, we find the optimal/average number of bits we require to transmit an information using a probability distribution applied to its set of events. If we use the probability distribution of <em>q</em> to devise a code for the information from <em>p</em>, then the optimal/average number of bits we’ll need is known as the cross entropy. This is quite insightful. Cross-entropy tells us how different the 2 distributions are. The closer they are, the closer will their entropies be.</p>

<p>Hence, it is represented as follows.</p>

<script type="math/tex; mode=display">H(p, q) = - \sum_{i} p_i\log_2 q_i</script>

<p>Let’s write some code to better understand this.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">obtained_probability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>
<span class="n">true_probability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">dirichlet</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">))</span>

<span class="k">assert</span> <span class="nb">sum</span><span class="p">(</span><span class="n">obtained_probability</span><span class="p">),</span> <span class="nb">sum</span><span class="p">(</span><span class="n">true_probability</span><span class="p">)</span> <span class="o">==</span> <span class="mf">1.0</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Obtained Probability: "</span><span class="p">,</span> <span class="n">obtained_probability</span><span class="p">)</span>
<span class="c"># [0.22374847 0.01351315 0.26085442 0.04087474 0.05121301 0.04455142 0.14301258 0.22223221]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"True Probability: "</span><span class="p">,</span> <span class="n">true_probability</span><span class="p">)</span>
<span class="c"># [0.29786589 0.01631927 0.00065958 0.27396781 0.11610311 0.10891934 0.01762108 0.16854391]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">),</span> <span class="n">obtained_probability</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"r"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.35</span><span class="p">,</span> <span class="n">true_probability</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.35</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p align="center">
<img src="http://oi65.tinypic.com/29gkg2e.jpg" alt="" height="350px" width="450px" />
</p>

<p>All we’ve done till now is generate 2 random probability distributions such that they add up to 1 and plotted them. Below, we’ve written certain routines to calculate entropies and cross-entropies for these 2 distributions.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">entropy</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">sum</span><span class="p">([</span><span class="n">x_i</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="nb">sum</span><span class="p">([</span><span class="n">p_i</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">q_i</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">p_i</span><span class="p">,</span> <span class="n">q_i</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)])</span>

<span class="n">entropy</span><span class="p">(</span><span class="n">true_probability</span><span class="p">)</span> <span class="c"># 2.380762398444909</span>
<span class="n">entropy</span><span class="p">(</span><span class="n">obtained_probability</span><span class="p">)</span> <span class="c"># 2.5644798655740377</span>
<span class="n">cross_entropy</span><span class="p">(</span><span class="n">true_probability</span><span class="p">,</span> <span class="n">obtained_probability</span><span class="p">)</span> <span class="c"># 3.4115389365100524</span>
<span class="n">cross_entropy</span><span class="p">(</span><span class="n">true_probability</span><span class="p">,</span> <span class="n">true_probability</span><span class="p">)</span> <span class="c"># 2.380762398444909</span>
</code></pre></div></div>

<p>Notice, the cross-entropy will always be higher than the individual entropies since we’ve performed the encoding using the wrong distribution. Also, when both the distributions are same, the cross-entropy and entropy are equal to each other.</p>

<h2 id="kullback-leibler-divergence">Kullback-Leibler divergence</h2>

<p>Yayy! So, we’ve accomplished most of the crucial stuff required to understand KL divergence.</p>

<p>The KL divergence between two probability distributions is just the number of extra bits we need if we encode the information represented by one distribution using the probability distribution of the other. It is the difference between cross entropy and entropy. Yes, that’s it. You can take a guess regarding how it is mathematically represented and you’ll probably be right.</p>

<script type="math/tex; mode=display">KLD = cross entropy - entropy = H(p, q) - H(p) \\
= (- \sum_{i} p_i\log_2 q_i) - (- \sum_{i} p_i\log_2 p_i)\\
= \sum_{i} p_i\log_2 p_i - \sum_{i} p_i\log_2 q_i\\
\text{using } \log(x \div y) = \log(x) - \log(y) \text{ we get,}\\
KLD = \sum_{i} p_i log_2 (\frac{p_i}{q_i})\\</script>

<p>Lesser the cross-entropy, lesser will be the KL divergence. The term <em>divergence</em> in itself is quite self-explanatory. The factor by which one distribution diverges from the other is basically what KL divergence is.</p>

<p>Writing the code for it should be easy now.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kld</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">cross_entropy</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span> <span class="o">-</span> <span class="n">entropy</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="n">kld</span><span class="p">(</span><span class="n">true_probability</span><span class="p">,</span> <span class="n">obtained_probability</span><span class="p">)</span> <span class="c"># 1.0307765380651435</span>
</code></pre></div></div>

<p>In our case, the divergence comes out to be a factor of 1.0307765380651435.</p>

<p>I hope that you’ve clearly understood KL divergence. See ya next time! probably with the explanation of a different topic which I personally find hard to understand.</p>

  </article>


</section>


<!-- begin bottom -->


    </div><!-- /content -->
  </div><!-- /container -->
  <div class="block">
    <div id="disqus_thread"></div>
    <script>
      /**
       *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
       *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
       */
      /*
      var disqus_config = function () {
          this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
          this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
      };
      */
      (function() {  // DON'T EDIT BELOW THIS LINE
          var d = document, s = d.createElement('script');
          
          s.src = 'https://blog-7snynlmin2.disqus.com/embed.js';
          
          s.setAttribute('data-timestamp', +new Date());
          (d.head || d.body).appendChild(s);
      })();
  </script>
  </div>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-63062556-4', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>

